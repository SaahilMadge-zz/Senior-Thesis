{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is an implementation of the End-to-End Memory Network as defined by Sukhbaatar, et al. \n",
    "# We use k=1, i.e. have only one computational step in the network\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import fbTaskReader as taskReader\n",
    "reload(taskReader)\n",
    "\n",
    "# Get the input matrices for the facebook tasks\n",
    "taskFilePaths = [\"/Users/SaahilM/Documents/Princeton/Academics/Thesis/Data/tasks_1-20_v1-2/en/qa1_single-supporting-fact\"]\n",
    "task1 = taskReader.VectorizeTask(taskFilePaths[0])\n",
    "# task1.printStory()\n",
    "qaPairs = task1.getTextQuestionPairs()\n",
    "# print task1.getTextQuestionPairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 19, 50)\n"
     ]
    }
   ],
   "source": [
    "# theano.config.compute_test_value = 'warn'\n",
    "\n",
    "# Number of training examples\n",
    "N = task1.getNumTrainingExamples()\n",
    "# Number of words in our dictionary\n",
    "V = task1.getNumWords()\n",
    "# Dimension to encode our information. They use 20 for independent training\n",
    "d = 50\n",
    "\n",
    "print(N,V,d)\n",
    "\n",
    "# Input X is the collection of sentences, Vx(text_length) matrix\n",
    "# Input q is our query, a vector of size Vx1\n",
    "# The actual result is y, a vector of size Vx1\n",
    "X = T.lmatrix('X')\n",
    "q = T.lrow('q')\n",
    "y = T.lvector('y')\n",
    "\n",
    "X.tag.test_value = np.zeros((2, V), dtype=np.int64)\n",
    "q.tag.test_value = np.zeros(V, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a weight matrix of given size. \n",
    "# The matrix is initialized randomly with Gaussian distribution \n",
    "# with mean=0 and \\sigma=0.1\n",
    "def initializeWeightMatrix(in_size, out_size):\n",
    "    return theano.shared(0.1 * np.random.randn(in_size, out_size))\n",
    "\n",
    "# Create a bias vector of all zeros of given size\n",
    "def initializeBiasVector(size):\n",
    "    return theano.shared(np.zeros(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50 19]\n"
     ]
    }
   ],
   "source": [
    "# Initialize all our parameters, given our dimensions.\n",
    "# A is the first matrix used to embed our input. It has size dxV\n",
    "# B is the matrix used to embed the query. It has size dxV\n",
    "# C is the next matrix used to embed our input. It has size dxV\n",
    "# W is the final matrix. Takes output O and produces result R. It has size dxd\n",
    "def initializeParams(d, V):\n",
    "    A = initializeWeightMatrix(V,d)\n",
    "    B = initializeWeightMatrix(V,d)\n",
    "    C = initializeWeightMatrix(V,d)\n",
    "    W = initializeWeightMatrix(d,V)\n",
    "#     A = theano.shared(initializeWeightMatrix(d, V))\n",
    "#     B = theano.shared(initializeWeightMatrix(d, V))\n",
    "#     C = theano.shared(initializeWeightMatrix(d, V))\n",
    "#     W = theano.shared(initializeWeightMatrix(V, d))\n",
    "    return A, B, C, W\n",
    "\n",
    "A, B, C, W = initializeParams(d, V)\n",
    "weightMatrices = [A, B, C, W]\n",
    "print(W.shape.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the computational step\n",
    "# Given input matrix X, query q, and weight matrices, we perform a computational step,\n",
    "# also known as a \"hop\". Let M be the number of sentences\n",
    "def hopComputation(X, q, A, B, C, W):\n",
    "    # m_i = Ax_i\n",
    "    mem_matrix = X.dot(A) #dimension (MxV)x(Vxd) = Mxd\n",
    "    # u = Bq\n",
    "    u = q.dot(B) #dimension (1xV)x(Vxd) = 1xd\n",
    "    # p_i = softmax(u^T m_i)\n",
    "    probs = T.nnet.softmax(mem_matrix.dot(u.T)) #dimension (Mxd)x(dx1) = Mx1\n",
    "    # C_i = Cx_i\n",
    "    c_embedded = X.dot(C) #dimension (MxV)x(Vxd) = Mxd\n",
    "    # output = sum of c_matrix * probs\n",
    "    o = (probs * c_embedded).sum(axis = 0) #dimension = 1xd\n",
    "    # result = softmax(W(o+u))\n",
    "    w_embedded = (o + u).dot(W) #dimension (1xd)x(dxV) = 1xV\n",
    "    \n",
    "    result = T.nnet.softmax(w_embedded)\n",
    "    return result\n",
    "    \n",
    "#     # m_i = Ax_i\n",
    "#     mem_matrix = A.dot(X)  #dimension dxM\n",
    "#     # u = Bq\n",
    "#     u = B.dot(q) #dimension dx1\n",
    "#     # p_i = softmax(u^T m_i)\n",
    "#     probs = T.nnet.softmax(u.T.dot(mem_matrix)) #dimension \n",
    "#     # C_i = Cx_i\n",
    "#     c_embedded = C.dot(X)\n",
    "#     # output = sum of c_matrix * probs\n",
    "#     o = (c_embedded * probs).sum(axis=1)\n",
    "#     # result = softmax(W(o+u))\n",
    "#     w_embedded = W.dot(o)\n",
    "#     w_summed = (w_embedded + u).T\n",
    "    \n",
    "#     # NOTE: MUST TRANSPOSE B/C IN THE PAPER THEY HAVE IT AS A COLUMN VECTOR BUT THEANO\n",
    "#     # NEEDS A ROW VECTOR. TOOK FOREVER TO FIGURE THIS OUT\n",
    "#     result = T.nnet.softmax(w_embedded.T)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = hopComputation(X, q, A, B, C, W)\n",
    "y_hat.tag.test_value = np.random.randn(1,V)\n",
    "loss = T.nnet.categorical_crossentropy(y_hat, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "def inspect_inputs(i, node, fn):\n",
    "    print(i, node, \"input(s) value(s):\", fn.inputs, end='')\n",
    "\n",
    "def inspect_outputs(i, node, fn):\n",
    "    print(\" output(s) value(s):\", fn.outputs)\n",
    "    \n",
    "def detect_nan(i, node, fn):\n",
    "    for output in fn.outputs:\n",
    "        if (not isinstance(output[0], np.random.RandomState) and\n",
    "            np.isnan(output[0]).any()):\n",
    "            print('*** NaN detected ***')\n",
    "            theano.printing.debugprint(node)\n",
    "            print('Inputs : %s' % [input[0] for input in fn.inputs])\n",
    "            print('Outputs: %s' % [output[0] for output in fn.outputs])\n",
    "            break\n",
    "\n",
    "# Learning rate (chosen to be 0.01)\n",
    "epsilon = 0.01\n",
    "\n",
    "# This function trains our neural net, using stochastic gradient descent.\n",
    "def train_MemNN(loss, X, q, y):\n",
    "    update_weights = []\n",
    "    for weightMatrix in weightMatrices:\n",
    "        update = T.grad(loss, weightMatrix)\n",
    "        update_weights.append((weightMatrix, weightMatrix - update * epsilon))\n",
    "    train_MemNN_func = theano.function(inputs=[X,q,y], outputs=loss, updates=update_weights, \n",
    "                        mode=theano.compile.MonitorMode(\n",
    "#                             pre_func=inspect_inputs,\n",
    "                            post_func=detect_nan))\n",
    "    return train_MemNN_func\n",
    "\n",
    "train_MemNN_func = train_MemNN(loss, X, q, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1451.4784311230917, 1442.0706640654698, 1440.5655623315288, 1439.7801899730835, 1439.3234919014317, 1439.0423984872073, 1438.8616768891281, 1438.7413176637874, 1438.6589226290826, 1438.6013788093883, 1438.5606850993593, 1438.5317684158449, 1438.5112955040249, 1438.4970025203079, 1438.4873038911555, 1438.4810570878656, 1438.4774172495777, 1438.4757450760656, 1438.4755471074002, 1438.4764361342834, 1438.478104369196, 1438.4803048522433, 1438.4828382612118, 1438.4855433272128, 1438.4882896969798, 1438.4909724852603, 1438.4935080172556, 1438.4958304265738, 1438.4978888819062, 1438.499645286666, 1438.501072342907, 1438.5021519024831, 1438.5028735498042, 1438.5032333752215, 1438.5032329081869, 1438.5028781864744, 1438.5021789428206, 1438.5011478939373, 1438.4998001195088, 1438.4981525209253, 1438.4962233507765, 1438.4940318055187, 1438.4915976745579, 1438.4889410397611, 1438.4860820200099, 1438.4830405559969, 1438.4798362308627, 1438.4764881226652, 1438.4730146850932, 1438.4694336530372, 1438.4657619700724, 1438.4620157350055, 1438.4582101650512, 1438.4543595733071, 1438.4504773584738, 1438.446576004945, 1438.4426670916196, 1438.4387613078907, 1438.4348684754889, 1438.4309975750241, 1438.4271567761143, 1438.4233534702635, 1438.4195943056297, 1438.415885223048, 1438.4122314926983, 1438.4086377509323, 1438.4051080368818, 1438.4016458284407, 1438.3982540774655, 1438.3949352438622, 1438.3916913284913, 1438.3885239047486, 1438.3854341487247, 1438.3824228679423, 1438.3794905286031, 1438.3766372814221, 1438.3738629859909, 1438.3711672337749, 1438.3685493697747, 1438.366008512887, 1438.3635435750678, 1438.3611532793564, 1438.358836176772, 1438.3565906622568, 1438.3544149896409, 1438.3523072857347, 1438.3502655636087, 1438.3482877350991, 1438.3463716226256, 1438.3445149703361, 1438.3427154546418, 1438.3409706941877, 1438.339278259275, 1438.3376356807855, 1438.3360404586119, 1438.3344900696745, 1438.332981975436, 1438.3315136290821, 1438.3300824822313, 1438.3286859913023]\n"
     ]
    }
   ],
   "source": [
    "def train_model(train_data, epochs=100):\n",
    "    train_errors = []\n",
    "    for i in xrange(epochs):\n",
    "        error = 0\n",
    "#         print(train_data[0])\n",
    "        for textQuestionPair in train_data:\n",
    "            train_X = textQuestionPair[\"text\"]\n",
    "#             print(train_X.shape)\n",
    "            train_q = textQuestionPair[\"question\"]\n",
    "#             print(train_q.shape)\n",
    "            train_y = textQuestionPair[\"answer\"]\n",
    "#             print(train_y.shape)\n",
    "#             print train_y.dtype\n",
    "#             train_X = np.array(textQuestionPair[\"text\"])\n",
    "#             train_q = np.array(textQuestionPair[\"question\"])\n",
    "#             train_y = np.array(textQuestionPair[\"answer\"])\n",
    "            cur_loss = train_MemNN_func(train_X, train_q, train_y)\n",
    "            error += cur_loss\n",
    "        train_errors.append(error)\n",
    "    return train_errors\n",
    "\n",
    "train_errors = train_model(qaPairs)\n",
    "print(train_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Notes from Google Brain Talk\n",
    "\n",
    "First research done with convolutional neural net (Hinton 2012)\n",
    "NEW RESEARCH\n",
    "    - Each layer is differentiable function (potentially non-linear)\n",
    "    - Last layer is softmax and loss is cross-entropy\n",
    "    - Trained by mini-batch SGD\n",
    "    - Several tricks: batch normalization (Ioffe 2015), use ReLU for non-linearities, \n",
    "        several layers of convolutions at multiple scales, max-pooling, full-connect, \n",
    "        GPU for everything, and multiple replicas (50-100) talking to parameter server\n",
    "\n",
    "Representing Words - Classical View\n",
    "    - Classical way is one-hot word vectors (think of this as everything being vertices of the hypercube)\n",
    "        - this way, every pair of words is equally far apart \n",
    "    - Better way is to put them inside the hypercube (word2vec, for example)\n",
    "    - How to get there? Train word embeddings from text corpus\n",
    "        - Online pass over text corpus\n",
    "            - word2vec\n",
    "            - randomly pick a word in the corpus and a nearby word in the text \n",
    "            (like locational nearby, not vector nearby)\n",
    "            - move the corresponding embeddings nearer\n",
    "            - uses skip-gram stuff\n",
    "        - Collect co-occurrence statistics\n",
    "            - GloVe\n",
    "            - Compute pointwise mutual information between words\n",
    "            - Move embeddings to estimate them by dot product\n",
    "\n",
    "Language Modeling with Deep Learning\n",
    "    - Given a sequence of tokens, maximize its joint likelihood p(y_1, y_2, ..., y_T)\n",
    "    - Factorize like this: \\product_t p(y_t | y_1,...,y_{T-1})\n",
    "    - Classical approach: use n-grams to simplify and just count them! \n",
    "        - but this doesn't take into account long-term dependencies and can't generalize to word combos\n",
    "        that can occur frequently but u haven't seen\n",
    "    - Instead: condition on some function h of previous input and use RNNs\n",
    "        \\product_t p(y_t | h_t) with h_t = p (y_t | y_1,...,y_{t-1})\n",
    "        - Here, words are not one-hot encoded but have real word embeddings\n",
    "    - LSTMs work better for our long-term dependencies\n",
    "    \n",
    "    - sequence-to-sequence framework by Sutskever\n",
    "    \n",
    "Image Captioning Experiments\n",
    "    - A recent dataset started it all: MS-COCO dataset\n",
    "        - 75k training images\n",
    "        - 5k evaluation images\n",
    "        - each image has 5 different captions\n",
    "    - Image model Google LeNet(winner of 2014 challenge)\n",
    "    - Caption model is single-layer LSTM with 512 hidden units\n",
    "    - Words have embeddings of size 512\n",
    "    - Small dictionary of 8857 words\n",
    "    - Evaluate results: must compare word counts\n",
    "\n",
    "One More Trick: Scheduled Sampling\n",
    "    - Statistical Machine Translation (same model can be used for image captioning)\n",
    "    - Inference of Sequence Prediction Models with RNNs\n",
    "        - Can use beam search for sampling, but beam size must be small (< 20) for RNNs\n",
    "    - A Sampling approach for training RNNs (sometimes you should show it the true word) instead\n",
    "    of the previous predicted word. i.e. at time t instead of showing sample(t-1) show it true(t-1)\n",
    "    \n",
    "    - but must be very careful when you do this\n",
    "    \n",
    "Conclusions\n",
    "    - Image caption is one more application of the sequence-to-sequence framework\n",
    "    - It is important, during training, to expose the model to diverse situations it can encounter at inference time\n",
    "    - Sampling from the model provides this diversity\n",
    "    - Only sampling from the model during training is too hard\n",
    "    - \"Curriculum learning\" is a reasonable approach to go from completely guided mode towards a \n",
    "    mode that is similar to inference\n",
    "    - Good performance on a few tasks\n",
    "        - for the image caption competition, a few of these models were ensembled and they won!\n",
    "    \n",
    "NOTE: LSTMs are bad after ~50 or so. Also read about \"hyperparameter training\"\n",
    "  \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
