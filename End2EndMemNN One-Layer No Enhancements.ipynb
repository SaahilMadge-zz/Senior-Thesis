{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is an implementation of the End-to-End Memory Network as defined by Sukhbaatar, et al. \n",
    "# We use k=1, i.e. have only one computational step in the network\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import fbTaskReader as taskReader\n",
    "reload(taskReader)\n",
    "\n",
    "# Get the input matrices for the facebook tasks\n",
    "taskFilePaths = [\"/Users/SaahilM/Documents/Princeton/Academics/Thesis/Data/tasks_1-20_v1-2/en/qa1_single-supporting-fact\"]\n",
    "task1 = taskReader.VectorizeTask(taskFilePaths[0])\n",
    "# task1.printStory()\n",
    "qaPairs = task1.getTextQuestionPairs()\n",
    "# print task1.getTextQuestionPairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 19, 20)\n"
     ]
    }
   ],
   "source": [
    "# theano.config.compute_test_value = 'warn'\n",
    "\n",
    "# Number of training examples\n",
    "N = task1.getNumTrainingExamples()\n",
    "# Number of words in our dictionary\n",
    "V = task1.getNumWords()\n",
    "# Dimension to encode our information. They use 20 for independent training\n",
    "d = 20\n",
    "\n",
    "print(N,V,d)\n",
    "\n",
    "# Input X is the collection of sentences, Vx(text_length) matrix\n",
    "# Input q is our query, a vector of size Vx1\n",
    "# The actual result is y, a vector of size Vx1\n",
    "X = T.lmatrix('X')\n",
    "q = T.lrow('q')\n",
    "y = T.lvector('y')\n",
    "\n",
    "X.tag.test_value = np.zeros((2, V), dtype=np.int64)\n",
    "q.tag.test_value = np.zeros(V, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a weight matrix of given size. \n",
    "# The matrix is initialized randomly with Gaussian distribution \n",
    "# with mean=0 and \\sigma=0.1\n",
    "def initializeWeightMatrix(in_size, out_size):\n",
    "    return 0.1 * np.random.randn(in_size, out_size)\n",
    "\n",
    "# Create a bias vector of all zeros of given size\n",
    "def initializeBiasVector(size):\n",
    "    return np.zeros(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20 19]\n"
     ]
    }
   ],
   "source": [
    "# Initialize all our parameters, given our dimensions.\n",
    "# A is the first matrix used to embed our input. It has size dxV\n",
    "# B is the matrix used to embed the query. It has size dxV\n",
    "# C is the next matrix used to embed our input. It has size dxV\n",
    "# W is the final matrix. Takes output O and produces result R. It has size dxd\n",
    "def initializeParams(d, V):\n",
    "    A = theano.shared(initializeWeightMatrix(V, d))\n",
    "    B = theano.shared(initializeWeightMatrix(V, d))\n",
    "    C = theano.shared(initializeWeightMatrix(V, d))\n",
    "    W = theano.shared(initializeWeightMatrix(d, V))\n",
    "#     A = theano.shared(initializeWeightMatrix(d, V))\n",
    "#     B = theano.shared(initializeWeightMatrix(d, V))\n",
    "#     C = theano.shared(initializeWeightMatrix(d, V))\n",
    "#     W = theano.shared(initializeWeightMatrix(V, d))\n",
    "    return A, B, C, W\n",
    "\n",
    "A, B, C, W = initializeParams(d, V)\n",
    "weightMatrices = [A, B, C, W]\n",
    "print(W.shape.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the computational step\n",
    "# Given input matrix X, query q, and weight matrices, we perform a computational step,\n",
    "# also known as a \"hop\". Let M be the number of sentences\n",
    "def hopComputation(X, q, A, B, C, W):\n",
    "    # m_i = Ax_i\n",
    "    mem_matrix = X.dot(A) #dimension (MxV)x(Vxd) = Mxd\n",
    "    # u = Bq\n",
    "    u = q.dot(B) #dimension (1xV)x(Vxd) = 1xd\n",
    "    # p_i = softmax(u^T m_i)\n",
    "    probs = T.nnet.softmax(mem_matrix.dot(u.T)) #dimension (Mxd)x(dx1) = Mx1\n",
    "    # C_i = Cx_i\n",
    "    c_embedded = X.dot(C) #dimension (MxV)x(Vxd) = Mxd\n",
    "    # output = sum of c_matrix * probs\n",
    "    o = (probs * c_embedded).sum(axis = 0) #dimension = 1xd\n",
    "    # result = softmax(W(o+u))\n",
    "    w_embedded = (o + u).dot(W) #dimension (1xd)x(dxV) = 1xV\n",
    "    \n",
    "    result = T.nnet.softmax(w_embedded)\n",
    "    return result\n",
    "    \n",
    "#     # m_i = Ax_i\n",
    "#     mem_matrix = A.dot(X)  #dimension dxM\n",
    "#     # u = Bq\n",
    "#     u = B.dot(q) #dimension dx1\n",
    "#     # p_i = softmax(u^T m_i)\n",
    "#     probs = T.nnet.softmax(u.T.dot(mem_matrix)) #dimension \n",
    "#     # C_i = Cx_i\n",
    "#     c_embedded = C.dot(X)\n",
    "#     # output = sum of c_matrix * probs\n",
    "#     o = (c_embedded * probs).sum(axis=1)\n",
    "#     # result = softmax(W(o+u))\n",
    "#     w_embedded = W.dot(o)\n",
    "#     w_summed = (w_embedded + u).T\n",
    "    \n",
    "#     # NOTE: MUST TRANSPOSE B/C IN THE PAPER THEY HAVE IT AS A COLUMN VECTOR BUT THEANO\n",
    "#     # NEEDS A ROW VECTOR. TOOK FOREVER TO FIGURE THIS OUT\n",
    "#     result = T.nnet.softmax(w_embedded.T)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = hopComputation(X, q, A, B, C, W)\n",
    "y_hat.tag.test_value = np.random.randn(1,V)\n",
    "loss = T.nnet.categorical_crossentropy(y_hat, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "def inspect_inputs(i, node, fn):\n",
    "    print(i, node, \"input(s) value(s):\", fn.inputs, end='')\n",
    "\n",
    "def inspect_outputs(i, node, fn):\n",
    "    print(\" output(s) value(s):\", fn.outputs)\n",
    "    \n",
    "def detect_nan(i, node, fn):\n",
    "    for output in fn.outputs:\n",
    "        if (not isinstance(output[0], np.random.RandomState) and\n",
    "            np.isnan(output[0]).any()):\n",
    "            print('*** NaN detected ***')\n",
    "            theano.printing.debugprint(node)\n",
    "            print('Inputs : %s' % [input[0] for input in fn.inputs])\n",
    "            print('Outputs: %s' % [output[0] for output in fn.outputs])\n",
    "            break\n",
    "\n",
    "# Learning rate (chosen to be 0.01)\n",
    "epsilon = 0.02\n",
    "\n",
    "# This function trains our neural net, using stochastic gradient descent.\n",
    "def train_MemNN(loss, X, q, y):\n",
    "    update_weights = []\n",
    "    for weightMatrix in weightMatrices:\n",
    "        update = T.grad(loss, weightMatrix)\n",
    "        update_weights.append((weightMatrix, weightMatrix - update * epsilon))\n",
    "    train_MemNN_func = theano.function(inputs=[X,q,y], outputs=loss, updates=update_weights, \n",
    "                        mode=theano.compile.MonitorMode(\n",
    "#                             pre_func=inspect_inputs,\n",
    "                            post_func=detect_nan))\n",
    "    return train_MemNN_func\n",
    "\n",
    "train_MemNN_func = train_MemNN(loss, X, q, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1580.9212362830456, 1580.9142271360688, 1580.9072776780388, 1580.9003930241536, 1580.8935773477128, 1580.88683370842, 1580.8801639117034, 1580.8735684044025, 1580.867046210851, 1580.8605949124765, 1580.8542106722548, 1580.8478883042715, 1580.8416213869082, 1580.8354024167777, 1580.8292229991941, 1580.8230740697823, 1580.8169461406171, 1580.8108295636011, 1580.8047148031951, 1580.7985927102197, 1580.7924547884427, 1580.7862934458187, 1580.7801022227213, 1580.7738759901872, 1580.767611111886, 1580.7613055647732, 1580.7549590143576, 1580.7485728417623, 1580.742150121265, 1580.735695548125, 1580.7292153181011, 1580.7227169613611, 1580.7162091348077, 1580.7097013781583, 1580.703203840307, 1580.6967269834522, 1580.6902812733892, 1580.683876864988, 1580.6775232922778, 1580.6712291726446, 1580.6650019345166, 1580.6588475773533, 1580.6527704719715, 1580.6467732080798, 1580.6408564945921, 1580.6350191164986, 1580.6292579504434, 1580.6235680390473, 1580.6179427223615, 1580.6123738225645, 1580.6068518765774, 1580.6013664094871, 1580.5959062404404, 1580.5904598116076, 1580.5850155301182, 1580.5795621125237, 1580.5740889213528, 1580.5685862836069, 1580.5630457816662, 1580.5574605079501, 1580.5518252758238, 1580.5461367804085, 1580.5403937045392, 1580.5345967664816, 1580.5287487077919, 1580.5228542211153, 1580.5169198196672, 1580.5109536514781, 1580.5049652632372, 1580.4989653200339, 1580.4929652886633, 1580.4869770933833, 1580.4810127540713, 1580.4750840175118, 1580.4692019929814, 1580.4633768036445, 1580.4576172648372, 1580.451930600017, 1580.4463222039778, 1580.4407954617727, 1580.4353516299761, 1580.429989785029, 1580.4247068412171, 1580.4194976383733, 1580.4143550972508, 1580.4092704378927, 1580.4042334544379, 1580.3992328376482, 1580.3942565349564, 1580.3892921366123, 1580.384327275593, 1580.3793500286438, 1580.3743493058339, 1580.3693152164026, 1580.3642393995542, 1580.3591153099067, 1580.3539384487417, 1580.348706533719, 1580.3434196016381, 1580.3380800405023]\n"
     ]
    }
   ],
   "source": [
    "def train_model(train_data, epochs=100):\n",
    "    train_errors = []\n",
    "    for i in xrange(epochs):\n",
    "        error = 0\n",
    "#         print(train_data[0])\n",
    "        for textQuestionPair in train_data:\n",
    "            train_X = textQuestionPair[\"text\"]\n",
    "#             print(train_X.shape)\n",
    "            train_q = textQuestionPair[\"question\"]\n",
    "#             print(train_q.shape)\n",
    "            train_y = textQuestionPair[\"answer\"]\n",
    "#             print(train_y.shape)\n",
    "#             print train_y.dtype\n",
    "#             train_X = np.array(textQuestionPair[\"text\"])\n",
    "#             train_q = np.array(textQuestionPair[\"question\"])\n",
    "#             train_y = np.array(textQuestionPair[\"answer\"])\n",
    "            cur_loss = train_MemNN_func(train_X, train_q, train_y)\n",
    "            error += cur_loss\n",
    "        train_errors.append(error)\n",
    "    return train_errors\n",
    "\n",
    "train_errors = train_model(qaPairs)\n",
    "print(train_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Notes from Google Brain Talk\n",
    "\n",
    "First research done with convolutional neural net (Hinton 2012)\n",
    "NEW RESEARCH\n",
    "    - Each layer is differentiable function (potentially non-linear)\n",
    "    - Last layer is softmax and loss is cross-entropy\n",
    "    - Trained by mini-batch SGD\n",
    "    - Several tricks: batch normalization (Ioffe 2015), use ReLU for non-linearities, \n",
    "        several layers of convolutions at multiple scales, max-pooling, full-connect, \n",
    "        GPU for everything, and multiple replicas (50-100) talking to parameter server\n",
    "\n",
    "Representing Words - Classical View\n",
    "    - Classical way is one-hot word vectors (think of this as everything being vertices of the hypercube)\n",
    "        - this way, every pair of words is equally far apart \n",
    "    - Better way is to put them inside the hypercube (word2vec, for example)\n",
    "    - How to get there? Train word embeddings from text corpus\n",
    "        - Online pass over text corpus\n",
    "            - word2vec\n",
    "            - randomly pick a word in the corpus and a nearby word in the text \n",
    "            (like locational nearby, not vector nearby)\n",
    "            - move the corresponding embeddings nearer\n",
    "            - uses skip-gram stuff\n",
    "        - Collect co-occurrence statistics\n",
    "            - GloVe\n",
    "            - Compute pointwise mutual information between words\n",
    "            - Move embeddings to estimate them by dot product\n",
    "\n",
    "Language Modeling with Deep Learning\n",
    "    - Given a sequence of tokens, maximize its joint likelihood p(y_1, y_2, ..., y_T)\n",
    "    - Factorize like this: \\product_t p(y_t | y_1,...,y_{T-1})\n",
    "    - Classical approach: use n-grams to simplify and just count them! \n",
    "        - but this doesn't take into account long-term dependencies and can't generalize to word combos\n",
    "        that can occur frequently but u haven't seen\n",
    "    - Instead: condition on some function h of previous input and use RNNs\n",
    "        \\product_t p(y_t | h_t) with h_t = p (y_t | y_1,...,y_{t-1})\n",
    "        - Here, words are not one-hot encoded but have real word embeddings\n",
    "    - LSTMs work better for our long-term dependencies\n",
    "    \n",
    "    - sequence-to-sequence framework by Sutskever\n",
    "    \n",
    "Image Captioning Experiments\n",
    "    - A recent dataset started it all: MS-COCO dataset\n",
    "        - 75k training images\n",
    "        - 5k evaluation images\n",
    "        - each image has 5 different captions\n",
    "    - Image model Google LeNet(winner of 2014 challenge)\n",
    "    - Caption model is single-layer LSTM with 512 hidden units\n",
    "    - Words have embeddings of size 512\n",
    "    - Small dictionary of 8857 words\n",
    "    - Evaluate results: must compare word counts\n",
    "\n",
    "One More Trick: Scheduled Sampling\n",
    "    - Statistical Machine Translation (same model can be used for image captioning)\n",
    "    - Inference of Sequence Prediction Models with RNNs\n",
    "        - Can use beam search for sampling, but beam size must be small (< 20) for RNNs\n",
    "    - A Sampling approach for training RNNs (sometimes you should show it the true word) instead\n",
    "    of the previous predicted word. i.e. at time t instead of showing sample(t-1) show it true(t-1)\n",
    "    \n",
    "    - but must be very careful when you do this\n",
    "    \n",
    "Conclusions\n",
    "    - Image caption is one more application of the sequence-to-sequence framework\n",
    "    - It is important, during training, to expose the model to diverse situations it can encounter at inference time\n",
    "    - Sampling from the model provides this diversity\n",
    "    - Only sampling from the model during training is too hard\n",
    "    - \"Curriculum learning\" is a reasonable approach to go from completely guided mode towards a \n",
    "    mode that is similar to inference\n",
    "    - Good performance on a few tasks\n",
    "        - for the image caption competition, a few of these models were ensembled and they won!\n",
    "    \n",
    "NOTE: LSTMs are bad after ~50 or so. Also read about \"hyperparameter training\"\n",
    "  \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
