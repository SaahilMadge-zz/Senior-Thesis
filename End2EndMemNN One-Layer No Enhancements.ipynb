{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is an implementation of the End-to-End Memory Network as defined by Sukhbaatar, et al. \n",
    "# We use k=1, i.e. have only one computational step in the network\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of training examples\n",
    "N = 1\n",
    "# Number of sentences in the input text\n",
    "M = 1\n",
    "# Number of words in our dictionary\n",
    "V = 1\n",
    "# Dimension to encode our information. They use 20 for independent training\n",
    "d = 20\n",
    "\n",
    "# Input X is the collection of sentences, NxVxM matrix\n",
    "# Input q is our query, a matrix of size NxV\n",
    "# The actual result is y\n",
    "X = T.matrix('X')\n",
    "q = T.matrix('q')\n",
    "y = T.lvector('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a weight matrix of given size. \n",
    "# The matrix is initialized randomly with Gaussian distribution \n",
    "# with mean=0 and \\sigma=0.1\n",
    "def initializeWeightMatrix(in_size, out_size):\n",
    "    return 0.1 * np.random.randn(in_size, out_size)\n",
    "\n",
    "# Create a bias vector of all zeros of given size\n",
    "def initializeBiasVector(size):\n",
    "    return numpy.zeros(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize all our parameters, given our dimensions.\n",
    "# A is the first matrix used to embed our input. It has size dxV\n",
    "# B is the matrix used to embed the query. It has size dxV\n",
    "# C is the next matrix used to embed our input. It has size dxV\n",
    "# W is the final matrix. Takes output O and produces result R. It has size dxd\n",
    "def initializeParams(d, V):\n",
    "    A = theano.shared(initializeWeightMatrix(d, V))\n",
    "    B = theano.shared(initializeWeightMatrix(d, V))\n",
    "    C = theano.shared(initializeWeightMatrix(d, V))\n",
    "    W = theano.shared(initializeWeightMatrix(d, d))\n",
    "    return A, B, C, W\n",
    "\n",
    "A, B, C, W = initializeParams(d, V)\n",
    "weightMatrices = [A, B, C, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the computational step\n",
    "# Given input matrix X, query q, and weight matrices, we perform a computational step,\n",
    "# also known as a \"hop\"\n",
    "def hopComputation(X, q, A, B, C, W):\n",
    "    # m_i = Ax_i\n",
    "    mem_matrix = A.dot(X)\n",
    "    # u = Bq\n",
    "    query_embedding = B.dot(q)\n",
    "    # p_i = softmax(q^T m_i)\n",
    "    probs = T.nnet.softmax(query_embedding.T.dot(mem_matrix))\n",
    "    # C_i = Cx_i\n",
    "    c_matrix = C.dot(X)\n",
    "    # output = sum of c_matrix * probs\n",
    "    o = (c_matrix * probs).sum(axis=1)\n",
    "    # result = Wo\n",
    "    result = W.dot(o)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat = hopComputation(X, q, A, B, C, W)\n",
    "loss = T.nnet.categorical_crossentropy(y_hat, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Learning rate (chosen to be 0.01)\n",
    "epsilon = 0.01\n",
    "\n",
    "# This function trains our neural net, using stochastic gradient descent.\n",
    "def train_MemNN(loss, X, q, y):\n",
    "    update_weights = []\n",
    "    for weightMatrix in weightMatrices:\n",
    "        update = T.grad(loss, weightMatrix)\n",
    "        update_weights.append((weightMatrix, weightMatrix - update * epsilon))\n",
    "    train_MemNN_func = theano.function(inputs=[X,q,y], outputs=loss, updates=update_weights)\n",
    "    return train_MemNN_func\n",
    "\n",
    "train_MemNN_func = train_MemNN(loss, X, q, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Notes from Google Brain Talk\n",
    "\n",
    "First research done with convolutional neural net (Hinton 2012)\n",
    "NEW RESEARCH\n",
    "    - Each layer is differentiable function (potentially non-linear)\n",
    "    - Last layer is softmax and loss is cross-entropy\n",
    "    - Trained by mini-batch SGD\n",
    "    - Several tricks: batch normalization (Ioffe 2015), use ReLU for non-linearities, \n",
    "        several layers of convolutions at multiple scales, max-pooling, full-connect, \n",
    "        GPU for everything, and multiple replicas (50-100) talking to parameter server\n",
    "\n",
    "Representing Words - Classical View\n",
    "    - Classical way is one-hot word vectors (think of this as everything being vertices of the hypercube)\n",
    "        - this way, every pair of words is equally far apart \n",
    "    - Better way is to put them inside the hypercube (word2vec, for example)\n",
    "    - How to get there? Train word embeddings from text corpus\n",
    "        - Online pass over text corpus\n",
    "            - word2vec\n",
    "            - randomly pick a word in the corpus and a nearby word in the text \n",
    "            (like locational nearby, not vector nearby)\n",
    "            - move the corresponding embeddings nearer\n",
    "            - uses skip-gram stuff\n",
    "        - Collect co-occurrence statistics\n",
    "            - GloVe\n",
    "            - Compute pointwise mutual information between words\n",
    "            - Move embeddings to estimate them by dot product\n",
    "\n",
    "Language Modeling with Deep Learning\n",
    "    - Given a sequence of tokens, maximize its joint likelihood p(y_1, y_2, ..., y_T)\n",
    "    - Factorize like this: \\product_t p(y_t | y_1,...,y_{T-1})\n",
    "    - Classical approach: use n-grams to simplify and just count them! \n",
    "        - but this doesn't take into account long-term dependencies and can't generalize to word combos\n",
    "        that can occur frequently but u haven't seen\n",
    "    - Instead: condition on some function h of previous input and use RNNs\n",
    "        \\product_t p(y_t | h_t) with h_t = p (y_t | y_1,...,y_{t-1})\n",
    "        - Here, words are not one-hot encoded but have real word embeddings\n",
    "    - LSTMs work better for our long-term dependencies\n",
    "    \n",
    "    - sequence-to-sequence framework by Sutskever\n",
    "    \n",
    "Image Captioning Experiments\n",
    "    - A recent dataset started it all: MS-COCO dataset\n",
    "        - 75k training images\n",
    "        - 5k evaluation images\n",
    "        - each image has 5 different captions\n",
    "    - Image model Google LeNet(winner of 2014 challenge)\n",
    "    - Caption model is single-layer LSTM with 512 hidden units\n",
    "    - Words have embeddings of size 512\n",
    "    - Small dictionary of 8857 words\n",
    "    - Evaluate results: must compare word counts\n",
    "\n",
    "One More Trick: Scheduled Sampling\n",
    "    - Statistical Machine Translation (same model can be used for image captioning)\n",
    "    - Inference of Sequence Prediction Models with RNNs\n",
    "        - Can use beam search for sampling, but beam size must be small (< 20) for RNNs\n",
    "    - A Sampling approach for training RNNs (sometimes you should show it the true word) instead\n",
    "    of the previous predicted word. i.e. at time t instead of showing sample(t-1) show it true(t-1)\n",
    "    \n",
    "    - but must be very careful when you do this\n",
    "    \n",
    "Conclusions\n",
    "    - Image caption is one more application of the sequence-to-sequence framework\n",
    "    - It is important, during training, to expose the model to diverse situations it can encounter at inference time\n",
    "    - Sampling from the model provides this diversity\n",
    "    - Only sampling from the model during training is too hard\n",
    "    - \"Curriculum learning\" is a reasonable approach to go from completely guided mode towards a \n",
    "    mode that is similar to inference\n",
    "    - Good performance on a few tasks\n",
    "        - for the image caption competition, a few of these models were ensembled and they won!\n",
    "    \n",
    "NOTE: LSTMs are bad after ~50 or so. Also read about \"hyperparameter training\"\n",
    "  \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
