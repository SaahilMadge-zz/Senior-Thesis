{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is an implementation of the End-to-End Memory Network as defined by Sukhbaatar, et al. \n",
    "# We use k=1, i.e. have only one computational step in the network\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from triple_reader import triple_reader\n",
    "from question_reader import question_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read in file as tensors\n",
    "text_file = (\"/Users/SaahilM/Documents/Princeton/Academics/Thesis/\"\n",
    "    \"Senior Thesis Code/ModifiedEntityGraph/prod/MCTest/production/MCTest/OCR_text/1/Triples/1-medium.txt\")\n",
    "    \n",
    "tr = triple_reader(text_file)\n",
    "# print tr.tripleList\n",
    "tensor = tr.tensor\n",
    "\n",
    "enMap = tr.enMap\n",
    "relMap = tr.relMap\n",
    "\n",
    "R = len(tensor)\n",
    "N = len(tensor[0])\n",
    "# dimension for encoding is arbitrary, we pick 20 here\n",
    "d = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 3726)\n",
      "(81, 3726)\n"
     ]
    }
   ],
   "source": [
    "tensor_stack = np.hstack(tuple(tensor))\n",
    "print(tensor_stack.shape)\n",
    "print(N, N*R)\n",
    "\n",
    "X = T.lmatrix('X')\n",
    "q = T.dmatrix('q')\n",
    "y = T.lmatrix('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "q_file = (\"/Users/SaahilM/Documents/Princeton/Academics/Thesis/Senior Thesis Code/\"\n",
    "\"ModifiedEntityGraph/prod/MCTest/production/MCTest/OCR_text/1/1-medium1-q.txt\")\n",
    "\n",
    "qr = question_reader(q_file)\n",
    "\n",
    "# print qr.numQuestions\n",
    "\n",
    "numTrainQ = int(qr.numQuestions*(float(2)/3))\n",
    "numTestQ = qr.numQuestions - numTrainQ\n",
    "# print numTrainQ\n",
    "# print numTestQ\n",
    "\n",
    "CHOICES_PER_Q = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a weight matrix of given size. \n",
    "# The matrix is initialized randomly with Gaussian distribution \n",
    "# with mean=0 and \\sigma=0.1\n",
    "def initializeWeightMatrix(in_size, out_size):\n",
    "    return theano.shared(0.1 * np.random.randn(in_size, out_size))\n",
    "\n",
    "# Create a bias vector of all zeros of given size\n",
    "def initializeBiasVector(size):\n",
    "    return theano.shared(np.zeros(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 20]\n"
     ]
    }
   ],
   "source": [
    "# Initialize all our parameters, given our dimensions.\n",
    "# Input matrix has shape Nx(N*R)\n",
    "# Query matrix has shape 5xnumQ\n",
    "# A is the first matrix used to embed our input. It has size dxN\n",
    "# B is the matrix used to embed the query. It has size dx5\n",
    "# C is the next matrix used to embed our input. It has size dxN\n",
    "# W is the final matrix. Takes output O and produces result w_embedded. It has size 5xd\n",
    "\n",
    "def initializeParams(d, N):\n",
    "    A = initializeWeightMatrix(d,N)\n",
    "    B = initializeWeightMatrix(d,CHOICES_PER_Q)\n",
    "    C = initializeWeightMatrix(d,N)\n",
    "    W = initializeWeightMatrix(CHOICES_PER_Q,d)\n",
    "    \n",
    "#     A = theano.shared(initializeWeightMatrix(d, V))\n",
    "#     B = theano.shared(initializeWeightMatrix(d, V))\n",
    "#     C = theano.shared(initializeWeightMatrix(d, V))\n",
    "#     W = theano.shared(initializeWeightMatrix(V, d))\n",
    "    return A, B, C, W\n",
    "\n",
    "A, B, C, W = initializeParams(d, N)\n",
    "weightMatrices = [A, B, C, W]\n",
    "print(W.shape.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the computational step\n",
    "# Given input matrix X, query q, and weight matrices, we perform a computational step,\n",
    "# also known as a \"hop\". Let M be the number of sentences\n",
    "def hopComputation(X, q, A, B, C, W):\n",
    "    #m_i = Ax_i\n",
    "    mem_matrix = A.dot(X) #dimension (dxN) x (Nx(NxR)) = dx(N*R)\n",
    "    #u = Bq\n",
    "    u = B.dot(q) #dimension (dx5) x (5xnumQ) = dxnumQ\n",
    "    #p_i = softmax(u^T m_i)\n",
    "    probs = T.nnet.softmax(u.T.dot(mem_matrix)) #dimension(numQxd)x(dx(N*R)) = numQx(N*R)\n",
    "    #C_i = Cx_i\n",
    "    c = C.dot(X) #dimension (dxN) x (Nx(NxR)) = dx(N*R)\n",
    "    o = c.dot(probs.T) #dimension (dx(N*R))x((N*R)xnumQ) = dxnumQ\n",
    "    \n",
    "    #w_embedded = Wo\n",
    "    w_embedded = W.dot(o).T #dimension (5xd)x(dxnumQ) = 5xnumQ.T = numQx5\n",
    "    \n",
    "    result = T.nnet.softmax(w_embedded)\n",
    "    return result\n",
    "    \n",
    "    #output = sum of c_matrix * probs\n",
    "#     o = (probs * c_embedded).sum(axis = 0)\n",
    "    #result = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_hat = hopComputation(X, q, A, B, C, W)\n",
    "loss = T.nnet.categorical_crossentropy(y_hat, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "def inspect_inputs(i, node, fn):\n",
    "    print(i, node, \"input(s) value(s):\", fn.inputs, end='')\n",
    "\n",
    "def inspect_outputs(i, node, fn):\n",
    "    print(\" output(s) value(s):\", fn.outputs)\n",
    "    \n",
    "def detect_nan(i, node, fn):\n",
    "    for output in fn.outputs:\n",
    "        if (not isinstance(output[0], np.random.RandomState) and\n",
    "            np.isnan(output[0]).any()):\n",
    "            print('*** NaN detected ***')\n",
    "            theano.printing.debugprint(node)\n",
    "            print('Inputs : %s' % [input[0] for input in fn.inputs])\n",
    "            print('Outputs: %s' % [output[0] for output in fn.outputs])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Learning rate (chosen to be 0.01)\n",
    "epsilon = 0.4\n",
    "\n",
    "# This function trains our neural net, using stochastic gradient descent.\n",
    "def train_MemNN(loss, X, q, y, y_hat):\n",
    "    update_weights = []\n",
    "    for weightMatrix in weightMatrices:\n",
    "        update = T.grad(loss, weightMatrix)\n",
    "        update_weights.append((weightMatrix, weightMatrix - update * epsilon))\n",
    "    train_MemNN_func = theano.function(inputs=[X,q,y], outputs=[loss,y_hat], updates=update_weights, \n",
    "                        mode=theano.compile.MonitorMode(\n",
    "#                             pre_func=inspect_inputs,\n",
    "                            post_func=detect_nan))\n",
    "    return train_MemNN_func\n",
    "\n",
    "train_MemNN_func = train_MemNN(loss, X, q, y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_model(in_vect, question, answers, epochs=100):\n",
    "    train_errors = []\n",
    "    y_hats = []\n",
    "    for i in xrange(epochs):\n",
    "        error = 0\n",
    "        [cur_loss, cur_yhat] = train_MemNN_func(in_vect, question, answers)\n",
    "        error += cur_loss\n",
    "#         print(error)\n",
    "        train_errors.append(error)\n",
    "        y_hats.append(cur_yhat)\n",
    "    return [train_errors, y_hats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6091778248987971, 1.6091753225475822, 1.6091728173995135, 1.6091703094290393, 1.6091677986105728, 1.6091652849184899, 1.6091627683271315, 1.6091602488108001, 1.6091577263437629, 1.6091552009002472, 1.6091526724544447, 1.6091501409805076, 1.6091476064525498, 1.6091450688446469, 1.6091425281308349, 1.60913998428511, 1.6091374372814284, 1.6091348870937074, 1.6091323336958219, 1.6091297770616069, 1.6091272171648554, 1.6091246539793198, 1.6091220874787096, 1.6091195176366919, 1.6091169444268922, 1.6091143678228912, 1.6091117877982279, 1.6091092043263964, 1.6091066173808468, 1.6091040269349857, 1.6091014329621731, 1.6090988354357254, 1.6090962343289128, 1.6090936296149587, 1.6090910212670422, 1.6090884092582936, 1.6090857935617973, 1.6090831741505907, 1.6090805509976618, 1.6090779240759523, 1.6090752933583539, 1.6090726588177104, 1.6090700204268158, 1.6090673781584146, 1.6090647319852014, 1.6090620818798198, 1.609059427814864, 1.6090567697628755, 1.6090541076963447, 1.6090514415877109, 1.60904877140936, 1.6090460971336253, 1.6090434187327882, 1.6090407361790755, 1.6090380494446603, 1.6090353585016612, 1.6090326633221432, 1.6090299638781151, 1.609027260141531, 1.6090245520842887, 1.6090218396782301, 1.6090191228951403, 1.6090164017067472, 1.6090136760847216, 1.6090109460006765, 1.6090082114261663, 1.609005472332687, 1.6090027286916753, 1.608999980474509, 1.6089972276525046, 1.6089944701969205, 1.6089917080789526, 1.6089889412697362, 1.6089861697403451, 1.6089833934617916, 1.6089806124050248, 1.6089778265409309, 1.6089750358403339, 1.6089722402739937, 1.6089694398126053, 1.6089666344268005, 1.6089638240871449, 1.6089610087641397, 1.6089581884282196, 1.6089553630497535, 1.6089525325990435, 1.608949697046324, 1.6089468563617626, 1.6089440105154591, 1.6089411594774434, 1.6089383032176783, 1.6089354417060555, 1.6089325749123984, 1.6089297028064586, 1.6089268253579192, 1.6089239425363895, 1.6089210543114092, 1.6089181606524448, 1.6089152615288902, 1.6089123569100674, 1.6089094467652236, 1.6089065310635324, 1.6089036097740936, 1.6089006828659316, 1.6088977503079953, 1.608894812069158, 1.6088918681182161, 1.6088889184238901, 1.6088859629548224, 1.6088830016795783, 1.6088800345666441, 1.6088770615844277, 1.608874082701258, 1.6088710978853837, 1.6088681071049735, 1.6088651103281151, 1.6088621075228153, 1.6088590986569984, 1.6088560836985077, 1.6088530626151025, 1.6088500353744597, 1.6088470019441719, 1.6088439622917472, 1.6088409163846096, 1.6088378641900971, 1.6088348056754622, 1.6088317408078709, 1.6088286695544021, 1.6088255918820475, 1.6088225077577101, 1.6088194171482058, 1.6088163200202603, 1.6088132163405096, 1.6088101060755002, 1.6088069891916876, 1.6088038656554364, 1.6088007354330183, 1.6087975984906135, 1.60879445479431, 1.608791304310101, 1.6087881470038863, 1.6087849828414704, 1.6087818117885644, 1.6087786338107812, 1.6087754488736399, 1.60877225694256, 1.6087690579828662, 1.6087658519597836, 1.6087626388384382, 1.6087594185838587, 1.608756191160972, 1.6087529565346059, 1.6087497146694862, 1.6087464655302377, 1.6087432090813827, 1.6087399452873412, 1.6087366741124285, 1.6087333955208571, 1.6087301094767348, 1.6087268159440629, 1.6087235148867376, 1.6087202062685488, 1.6087168900531783, 1.6087135662042011, 1.6087102346850837, 1.6087068954591817, 1.6087035484897432, 1.608700193739905, 1.6086968311726924, 1.6086934607510197, 1.6086900824376889, 1.6086866961953881, 1.6086833019866922, 1.6086798997740619, 1.608676489519842, 1.6086730711862633, 1.6086696447354385, 1.6086662101293641, 1.6086627673299183, 1.6086593162988612, 1.6086558569978333, 1.6086523893883558, 1.6086489134318285, 1.6086454290895311, 1.6086419363226199, 1.6086384350921294, 1.6086349253589707, 1.6086314070839294, 1.6086278802276679, 1.608624344750722, 1.6086208006135014, 1.6086172477762883, 1.6086136861992377, 1.6086101158423749, 1.6086065366655971, 1.6086029486286697, 1.6085993516912291, 1.6085957458127784, 1.6085921309526894, 1.6085885070701993, 1.6085848741244129, 1.608581232074298, 1.6085775808786893, 1.608573920496283, 1.6085702508856394, 1.6085665720051801, 1.6085628838131876, 1.6085591862678053, 1.6085554793270358, 1.6085517629487407, 1.6085480370906382, 1.6085443017103054, 1.608540556765174, 1.6085368022125317, 1.6085330380095204, 1.6085292641131355, 1.6085254804802245, 1.6085216870674888, 1.6085178838314782, 1.6085140707285939, 1.6085102477150861, 1.6085064147470536, 1.6085025717804415, 1.6084987187710427, 1.6084948556744938, 1.6084909824462785, 1.6084870990417217, 1.6084832054159932, 1.608479301524103, 1.6084753873209028, 1.6084714627610839, 1.6084675277991767, 1.6084635823895497, 1.6084596264864084, 1.6084556600437943, 1.6084516830155837, 1.6084476953554878, 1.6084436970170499, 1.6084396879536458, 1.6084356681184824, 1.6084316374645971, 1.6084275959448555, 1.6084235435119512, 1.6084194801184064, 1.6084154057165667, 1.6084113202586043, 1.6084072236965155, 1.6084031159821173, 1.6083989970670503, 1.608394866902775, 1.6083907254405718, 1.6083865726315389, 1.6083824084265914, 1.6083782327764624, 1.6083740456316979, 1.6083698469426595, 1.6083656366595205, 1.6083614147322665, 1.6083571811106929, 1.6083529357444057, 1.6083486785828178, 1.6083444095751491, 1.6083401286704269, 1.6083358358174806, 1.6083315309649457, 1.6083272140612581, 1.6083228850546547, 1.6083185438931731, 1.6083141905246487, 1.6083098248967147, 1.6083054469567999, 1.6083010566521274, 1.6082966539297148, 1.6082922387363712, 1.6082878110186962, 1.6082833707230799, 1.6082789177957009, 1.608274452182523, 1.6082699738292972, 1.6082654826815581, 1.6082609786846236, 1.6082564617835922, 1.6082519319233441, 1.6082473890485369, 1.6082428331036058, 1.6082382640327633, 1.608233681779994, 1.6082290862890583, 1.6082244775034864, 1.6082198553665794, 1.6082152198214072, 1.6082105708108072, 1.6082059082773821, 1.6082012321634991, 1.6081965424112883, 1.6081918389626411, 1.6081871217592087, 1.6081823907424009, 1.6081776458533827, 1.6081728870330756, 1.6081681142221547, 1.6081633273610463, 1.608158526389927, 1.6081537112487223, 1.608148881877105, 1.6081440382144929, 1.6081391802000478, 1.6081343077726735, 1.608129420871014, 1.6081245194334526, 1.6081196033981091, 1.6081146727028386, 1.6081097272852294, 1.6081047670826021, 1.6080997920320073, 1.6080948020702233, 1.6080897971337547, 1.6080847771588318, 1.6080797420814064, 1.6080746918371518, 1.6080696263614604, 1.6080645455894413, 1.6080594494559197, 1.6080543378954333, 1.6080492108422313, 1.6080440682302732, 1.6080389099932253, 1.6080337360644597, 1.6080285463770521, 1.6080233408637801, 1.6080181194571199, 1.6080128820892459, 1.6080076286920284, 1.6080023591970303, 1.6079970735355056, 1.6079917716383978, 1.6079864534363379, 1.6079811188596416, 1.6079757678383062, 1.6079704003020105, 1.6079650161801122, 1.6079596154016436, 1.6079541978953111, 1.6079487635894938, 1.6079433124122398, 1.6079378442912626, 1.6079323591539421, 1.6079268569273195, 1.6079213375380965, 1.607915800912632, 1.6079102469769397, 1.6079046756566873, 1.6078990868771905, 1.6078934805634144, 1.6078878566399692, 1.6078822150311072, 1.6078765556607211, 1.6078708784523412, 1.6078651833291331, 1.607859470213894, 1.6078537390290515, 1.6078479896966598, 1.6078422221383977, 1.6078364362755657, 1.6078306320290829, 1.6078248093194849, 1.6078189680669206, 1.6078131081911486, 1.607807229611536, 1.607801332247055, 1.6077954160162782, 1.6077894808373783, 1.6077835266281244, 1.6077775533058771, 1.6077715607875875, 1.6077655489897951, 1.6077595178286208, 1.6077534672197684, 1.6077473970785179, 1.6077413073197244, 1.6077351978578143, 1.6077290686067816, 1.6077229194801852, 1.607716750391146, 1.6077105612523424, 1.6077043519760075, 1.6076981224739266, 1.6076918726574323, 1.6076856024374022, 1.6076793117242545, 1.6076730004279449, 1.6076666684579637, 1.6076603157233305, 1.6076539421325926, 1.6076475475938197, 1.6076411320146016, 1.6076346953020426, 1.6076282373627597, 1.6076217581028778, 1.6076152574280256, 1.6076087352433315, 1.6076021914534215, 1.6075956259624138, 1.6075890386739131, 1.6075824294910104, 1.607575798316276, 1.6075691450517551, 1.6075624695989674, 1.6075557718588975, 1.6075490517319941, 1.6075423091181658, 1.607535543916774, 1.6075287560266318, 1.6075219453459972, 1.6075151117725699, 1.6075082552034854, 1.6075013755353118, 1.6074944726640448, 1.6074875464851019, 1.6074805968933195, 1.6074736237829466, 1.6074666270476397, 1.6074596065804596, 1.6074525622738656, 1.6074454940197098, 1.607438401709232, 1.6074312852330561, 1.6074241444811836, 1.6074169793429887, 1.6074097897072126, 1.6074025754619585, 1.6073953364946876, 1.6073880726922107, 1.6073807839406844, 1.6073734701256051, 1.6073661311318044, 1.6073587668434421, 1.6073513771440004, 1.6073439619162786, 1.6073365210423871, 1.6073290544037415, 1.6073215618810557, 1.6073140433543363, 1.6073064987028782, 1.6072989278052545, 1.6072913305393135, 1.6072837067821708, 1.6072760564102029, 1.607268379299041, 1.6072606753235639, 1.6072529443578922, 1.6072451862753798, 1.607237400948609, 1.6072295882493821, 1.607221748048715, 1.6072138802168301, 1.6072059846231488, 1.6071980611362839, 1.6071901096240331, 1.6071821299533715, 1.6071741219904427, 1.6071660856005523, 1.6071580206481606, 1.6071499269968739, 1.6071418045094354, 1.6071336530477209, 1.607125472472726, 1.6071172626445613, 1.6071090234224439, 1.6071007546646858, 1.6070924562286897, 1.6070841279709369, 1.6070757697469811, 1.6070673814114371, 1.6070589628179741, 1.6070505138193054, 1.607042034267179, 1.6070335240123701, 1.6070249829046688, 1.6070164107928733, 1.6070078075247782, 1.6069991729471664, 1.6069905069057979, 1.6069818092454009, 1.6069730798096602, 1.6069643184412081, 1.6069555249816134, 1.6069466992713723, 1.6069378411498949, 1.6069289504554971, 1.6069200270253881, 1.6069110706956593, 1.6069020813012744, 1.6068930586760568, 1.6068840026526776, 1.6068749130626454, 1.6068657897362937, 1.6068566325027687, 1.6068474411900173, 1.6068382156247745, 1.6068289556325515, 1.6068196610376231, 1.6068103316630138, 1.6068009673304862, 1.6067915678605265, 1.6067821330723326, 1.6067726627837988, 1.6067631568115035, 1.6067536149706949, 1.606744037075277, 1.6067344229377944, 1.60672477236942, 1.606715085179937, 1.6067053611777282, 1.6066956001697559, 1.6066858019615518, 1.6066759663571972, 1.6066660931593106, 1.6066561821690293, 1.6066462331859941, 1.606636246008333, 1.6066262204326454, 1.6066161562539834, 1.6066060532658357, 1.6065959112601107, 1.6065857300271187, 1.6065755093555525, 1.6065652490324718, 1.6065549488432829, 1.606544608571721, 1.6065342279998314, 1.6065238069079486, 1.6065133450746802, 1.6065028422768841, 1.6064922982896499, 1.6064817128862778, 1.6064710858382589, 1.6064604169152534, 1.6064497058850704, 1.6064389525136449, 1.6064281565650176, 1.606417317801311, 1.6064064359827086, 1.6063955108674304, 1.6063845422117111, 1.6063735297697752, 1.6063624732938144, 1.6063513725339624, 1.6063402272382701, 1.6063290371526819, 1.6063178020210087, 1.6063065215849033, 1.606295195583832, 1.6062838237550525, 1.6062724058335816, 1.6062609415521711, 1.6062494306412796, 1.6062378728290421, 1.6062262678412438, 1.6062146154012897, 1.6062029152301749, 1.6061911670464548, 1.6061793705662142, 1.6061675255030361, 1.6061556315679706, 1.6061436884695024, 1.6061316959135179, 1.6061196536032736, 1.6061075612393592, 1.606095418519667, 1.6060832251393551, 1.6060709807908122, 1.6060586851636214, 1.6060463379445253, 1.6060339388173868, 1.6060214874631524, 1.6060089835598135, 1.6059964267823683, 1.6059838168027798, 1.605971153289939, 1.6059584359096208, 1.6059456643244432, 1.6059328381938256, 1.6059199571739446, 1.605907020917692, 1.6058940290746273, 1.6058809812909351, 1.6058678772093775, 1.6058547164692474, 1.605841498706319, 1.6058282235528045, 1.6058148906372984, 1.6058014995847305, 1.6057880500163155, 1.6057745415494975, 1.6057609737979004, 1.6057473463712726, 1.605733658875431, 1.6057199109122078, 1.6057061020793919, 1.6056922319706703, 1.6056783001755706, 1.6056643062794005, 1.6056502498631873, 1.6056361305036142, 1.605621947772959, 1.6056077012390271, 1.605593390465089, 1.6055790150098108, 1.6055645744271869, 1.6055500682664714, 1.6055354960721067, 1.6055208573836512, 1.6055061517357077, 1.6054913786578453, 1.6054765376745284, 1.6054616283050329, 1.6054466500633739, 1.6054316024582191, 1.6054164849928112, 1.6054012971648819, 1.6053860384665675, 1.6053707083843214, 1.6053553063988262, 1.6053398319849039, 1.605324284611422, 1.6053086637412013, 1.6052929688309212, 1.6052771993310173, 1.6052613546855885, 1.6052454343322911, 1.6052294377022349, 1.6052133642198823, 1.6051972133029351, 1.6051809843622276, 1.6051646768016141, 1.6051482900178549, 1.6051318234004981, 1.6051152763317618, 1.6050986481864127, 1.6050819383316413, 1.6050651461269361, 1.6050482709239535, 1.6050313120663864, 1.6050142688898301, 1.6049971407216435, 1.6049799268808103, 1.6049626266777937, 1.6049452394143922, 1.6049277643835882, 1.604910200869397, 1.6048925481467093, 1.6048748054811326, 1.6048569721288288, 1.6048390473363463, 1.6048210303404526, 1.6048029203679579, 1.6047847166355402, 1.6047664183495616, 1.6047480247058847, 1.6047295348896811, 1.6047109480752402, 1.6046922634257696, 1.6046734800931917, 1.6046545972179385, 1.6046356139287388, 1.6046165293424013, 1.6045973425635927, 1.6045780526846132, 1.6045586587851626, 1.6045391599321022, 1.6045195551792144, 1.6044998435669531, 1.6044800241221897, 1.6044600958579518, 1.6044400577731581, 1.6044199088523456, 1.6043996480653893, 1.6043792743672174, 1.6043587866975191, 1.6043381839804418, 1.6043174651242886, 1.6042966290211991, 1.6042756745468305, 1.604254600560026, 1.6042334059024772, 1.6042120893983758, 1.6041906498540617, 1.6041690860576545, 1.6041473967786839, 1.6041255807677048, 1.6041036367559074, 1.6040815634547105, 1.6040593595553538, 1.604037023728472, 1.6040145546236604, 1.6039919508690306, 1.6039692110707546, 1.6039463338125939, 1.6039233176554186, 1.6039001611367145, 1.6038768627700772, 1.6038534210446884, 1.6038298344247859, 1.6038061013491101, 1.6037822202303449, 1.6037581894545365, 1.6037340073805002, 1.6037096723392088, 1.6036851826331655, 1.6036605365357572, 1.6036357322905967, 1.6036107681108351, 1.6035856421784658, 1.6035603526436022, 1.6035348976237382, 1.6035092752029865, 1.6034834834312943, 1.6034575203236388, 1.6034313838591983, 1.6034050719804946, 1.6033785825925222, 1.6033519135618377, 1.603325062715633, 1.6032980278407756, 1.6032708066828221, 1.6032433969450033, 1.6032157962871749, 1.603188002324742, 1.6031600126275438, 1.6031318247187123, 1.6031034360734904, 1.6030748441180129, 1.6030460462280545, 1.6030170397277339, 1.6029878218881801, 1.6029583899261546, 1.6029287410026307, 1.60289887222133, 1.6028687806272057, 1.602838463204882, 1.6028079168770417, 1.6027771385027614, 1.6027461248757895, 1.6027148727227714, 1.6026833787014114, 1.6026516393985757, 1.6026196513283331, 1.602587410929925, 1.6025549145656695, 1.6025221585187943, 1.6024891389911926, 1.6024558521011023, 1.6024222938807051, 1.6023884602736413, 1.6023543471324353, 1.6023199502158318, 1.602285265186036, 1.6022502876058522, 1.6022150129357269, 1.6021794365306739, 1.6021435536370923, 1.6021073593894697, 1.6020708488069577, 1.6020340167898215, 1.6019968581157591, 1.6019593674360777, 1.6019215392717248, 1.6018833680091695, 1.6018448478961247, 1.601805973037099, 1.6017667373887829, 1.6017271347552435, 1.6016871587829355, 1.6016468029555155, 1.601606060588437, 1.6015649248233388, 1.6015233886221993, 1.6014814447612509, 1.6014390858246399, 1.6013963041978356, 1.6013530920607422, 1.6013094413805411, 1.6012653439042137, 1.6012207911507526, 1.6011757744030297, 1.6011302846993198, 1.6010843128244436, 1.6010378493005224, 1.6009908843773291, 1.6009434080221945, 1.6008954099094654, 1.6008468794094823, 1.6007978055770484, 1.6007481771393697, 1.6006979824834255, 1.6006472096427549, 1.6005958462836092, 1.600543879690449, 1.6004912967507363, 1.6004380839389964, 1.6003842273000894, 1.600329712431666, 1.6002745244657415, 1.6002186480493477, 1.6001620673242019, 1.6001047659053438, 1.6000467268586607, 1.5999879326772497, 1.5999283652565393, 1.5998680058680903, 1.5998068351319976, 1.5997448329878061, 1.5996819786638357, 1.5996182506448295, 1.599553626637801, 1.5994880835359711, 1.599421597380664, 1.5993541433210274, 1.5992856955714334, 1.5992162273663957, 1.599145710912832, 1.5990741173394996, 1.5990014166433841, 1.5989275776328471, 1.5988525678672836, 1.5987763535930453, 1.5986988996753593, 1.5986201695259259, 1.5985401250259059, 1.5984587264439143, 1.5983759323486619, 1.5982916995158225, 1.5982059828286683, 1.5981187351720021, 1.5980299073188133, 1.5979394478091173, 1.597847302820282, 1.5977534160281963, 1.5976577284584605, 1.5975601783267785, 1.5974607008676296, 1.5973592281501734, 1.5972556888802896, 1.5971500081875059, 1.5970421073954397, 1.5969319037742589, 1.5968193102734811, 1.5967042352332712, 1.5965865820721823, 1.5964662489490591, 1.5963431283965845, 1.596217106923645, 1.5960880645833635, 1.5959558745032965, 1.5958204023738662, 1.59568150589063, 1.5955390341454359, 1.5953928269609363, 1.595242714162177, 1.5950885147782465, 1.5949300361659859, 1.5947670730467558, 1.5945994064460329, 1.5944268025241668, 1.5942490112851038, 1.5940657651479084, 1.5938767773638076, 1.5936817402589196, 1.5934803232798771, 1.593272170816094, 1.5930568997683512, 1.5928340968286117, 1.5926033154303019, 1.5923640723216483, 1.5921158437066509, 1.5918580608888988, 1.5915901053420241, 1.5913113031170882, 1.5910209184807931, 1.5907181466586573, 1.5904021055333417, 1.5900718261190294, 1.5897262415970657, 1.589364174654085, 1.5889843228095972, 1.5885852413527259, 1.588165323423858, 1.5877227766718085, 1.5872555957846224, 1.5867615300241957, 1.5862380446809201, 1.5856822750899733, 1.5850909714965271, 1.5844604325965195, 1.5837864249766822, 1.583064084882458, 1.5822877976857255, 1.5814510490086142, 1.5805462395467775, 1.579564453027982, 1.5784951631547341, 1.5773258603975955, 1.5760415725186649, 1.5746242428066033, 1.5730519158412126, 1.5712976601357451, 1.5693281271379267, 1.5671016021174438, 1.5645653373489401, 1.5616518611920085, 1.5582738131123537, 1.5543166446571848, 1.5496282297979349, 1.5440040436621343, 1.5371661785899486, 1.5287344279366832, 1.518189138289584, 1.5048317513567162, 1.4877677373711991, 1.4659820240901917, 1.4386548506872638, 1.4058946763946325, 1.3697295813568542, 1.3342616386144361, 1.3037296005433758, 1.2799157440770188, 1.2619602421323441, 1.2481153758130108, 1.2369851015944127, 1.2277346247038696, 1.2199057703600444, 1.2132327071460705, 1.2075362321142471, 1.2026752318183818, 1.1985278710335638, 1.1949858294275768, 1.1919532763248271, 1.1893469096489655, 1.1870957049637971, 1.1851400952818854, 1.1834307236280728, 1.1819270061568123, 1.1805957021938576, 1.1794096106745224, 1.1783464455607044, 1.1773878987695148, 1.176518875724164, 1.1757268792399063, 1.1750015159752243, 1.1743341019215237, 1.1737173469565949, 1.1731451021475214, 1.1726121567550352, 1.172114074617745, 1.171647061793063, 1.1712078590721495, 1.1707936543517019, 1.170402010911713, 1.1700308084806808, 1.1696781946191261, 1.169342544460152, 1.1690224272440766, 1.1687165783967375, 1.168423876148057, 1.1681433218822386, 1.1678740235662157, 1.167615181725842, 1.1673660775383272, 1.1671260626880402, 1.1668945506967165, 1.166671009490224, 1.1664549550058183, 1.1662459456774177, 1.1660435776642579, 1.1658474807105115, 1.1656573145422517, 1.1654727657231736, 1.1652935449031308, 1.1651193844039234, 1.1649500360953706, 1.1647852695219618, 1.1646248702463691, 1.1644686383809812, 1.1643163872830979, 1.1641679423926559, 1.164023140194588, 1.1638818272903797, 1.1637438595653586, 1.1636091014404024, 1.1634774251979569, 1.1633487103737554, 1.163222843206789, 1.1630997161409113, 1.1629792273723703, 1.1628612804383629, 1.1627457838420281, 1.1626326507102736, 1.1625217984808429, 1.1624131486157192]\n",
      "[[  2.16544968e-03   4.66227943e-01   1.34506729e-04   7.33657304e-03\n",
      "    5.24135528e-01]\n",
      " [  1.91947807e-01   2.14832688e-01   1.80966940e-01   1.96955540e-01\n",
      "    2.15297025e-01]\n",
      " [  3.78329177e-03   4.66492926e-01   3.13509464e-04   1.12926503e-02\n",
      "    5.18117622e-01]\n",
      " [  1.98620775e-01   2.02683372e-01   1.96432754e-01   1.99554353e-01\n",
      "    2.02708747e-01]]\n",
      "[[0 0 0 0 1]\n",
      " [0 0 0 1 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "in_vect = tensor_stack.astype(int)\n",
    "question = np.random.randn(5,numTrainQ).astype(int)\n",
    "answers = np.array([np.array([0,0,0,0,1]),np.array([0,0,0,1,0]),np.array([0,1,0,0,0]),np.array([1,0,0,0,0])]).astype(int)\n",
    "\n",
    "# print(in_vect)\n",
    "# print(question)\n",
    "# print(answers)\n",
    "# print(type(in_vect[0][0]))\n",
    "[train_errors, y_hats] = train_model(in_vect, question, answers, 1000)\n",
    "print(train_errors)\n",
    "print(y_hats[-1])\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
